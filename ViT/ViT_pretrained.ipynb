{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.pytorch_pretrained_vit import ViT\n",
    "from regularizer import prune_vit\n",
    "import dataloader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from copy import deepcopy\n",
    "\n",
    "from utils import AverageMeter, get_weights_copy\n",
    "\n",
    "import time\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/workspace/paper_works/work_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iter(net, loader, criterion, optimizer, epoch, device, reg_ratio=0., print_freq=200):\n",
    "    losses = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    net.train()\n",
    "    for i, (X, y) in enumerate(loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        pred = net(X)\n",
    "        loss = criterion(pred, y)\n",
    "        \n",
    "        losses.update(loss.data, X.size(0))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if reg_ratio > 0.:\n",
    "            idxs, lams = prune_vit(net, reg_ratio)\n",
    "        \n",
    "        if i % print_freq==0 or len(loader) - 1 == i:\n",
    "            print(' Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                                epoch, i, len(loader), loss=losses))\n",
    "    \n",
    "    if reg_ratio > 0.:\n",
    "        idxs, lams = prune_vit(net, reg_ratio)\n",
    "    \n",
    "    state_dicts = get_weights_copy(net, device)\n",
    "    \n",
    "    state = {\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': state_dicts,\n",
    "        'loss': losses.avg.detach().cpu().numpy().tolist(),\n",
    "    }\n",
    "    \n",
    "    if reg_ratio > 0.:\n",
    "        state['reg_idxs'] = idxs\n",
    "        state['reg_lams'] = lams\n",
    "    \n",
    "    return state\n",
    "\n",
    "def valid_iter(net, loader, criterion, optimizer, epoch, device, print_freq=100):\n",
    "    losses = AverageMeter()\n",
    "    net.eval()\n",
    "    for i, (X, y) in enumerate(loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        pred = net(X)\n",
    "        loss = criterion(pred, y)\n",
    "        \n",
    "        losses.update(loss.data, X.size(0))\n",
    "        \n",
    "        if i % print_freq==0 or len(loader) - 1 == i:\n",
    "            print(' Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                                epoch, i, len(loader), loss=losses))\n",
    "        \n",
    "    \n",
    "    state = {\n",
    "        'loss': losses.avg.detach().cpu().numpy().tolist(),\n",
    "    }\n",
    "    \n",
    "    return state\n",
    "\n",
    "def main(device, net_name, dataset_args, dataset_name, early_stopping_step,\n",
    "         lr, model_args, model_name, EPOCH=100, reg_ratio=0.):\n",
    "    \n",
    "    file_name = 'best_state.ptl'    \n",
    "    if reg_ratio == 0:\n",
    "        net = ViT(model_name, True, **model_args)\n",
    "    else:\n",
    "        net = ViT(model_name, False, **model_args)\n",
    "        state = torch.load('{}/{}_{}/{}'.format(BASE_PATH, dataset_name, net_name, file_name))\n",
    "        net.load_state_dict(state['state_dict'])\n",
    "#         net = ViT(model_name, True, **model_args)\n",
    "        file_name = 'best_state_ratio%d.ptl' % int(reg_ratio * 100)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    trainset, testset, num_classes = getattr(dataloader, dataset_name)(**dataset_args)\n",
    "    \n",
    "    best_state = None\n",
    "    best_loss = 9999999\n",
    "    early_stopping_idx = 0\n",
    "    for epoch in range(EPOCH):\n",
    "        early_stopping_idx += 1\n",
    "        train_state = train_iter(net, trainset, criterion, optimizer, epoch, device, reg_ratio)\n",
    "        valid_state = valid_iter(net, testset, criterion, optimizer, epoch, device)\n",
    "\n",
    "        if best_loss > valid_state['loss']:\n",
    "            best_loss = valid_state['loss']\n",
    "            best_state = train_state\n",
    "\n",
    "            early_stopping_idx = 0\n",
    "\n",
    "        if early_stopping_idx > early_stopping_step:\n",
    "            print('early stopping.')\n",
    "            break\n",
    "\n",
    "    if not os.path.exists('{}/{}_{}/'.format(BASE_PATH, dataset_name, net_name)):\n",
    "        os.mkdir('{}/{}_{}/'.format(BASE_PATH, dataset_name, net_name))\n",
    "\n",
    "    torch.save(best_state, '{}/{}_{}/{}'.format(BASE_PATH, dataset_name, net_name, file_name))\n",
    "    \n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using augmented CIFAR 100.\n",
      "Files already downloaded and verified\n",
      " Epoch: [0][0/391]\tLoss 1.1289 (1.1289)\t\n",
      " Epoch: [0][200/391]\tLoss 1.6259 (1.4537)\t\n",
      " Epoch: [0][390/391]\tLoss 1.4008 (1.4395)\t\n",
      " Epoch: [0][0/79]\tLoss 2.1181 (2.1181)\t\n",
      " Epoch: [0][78/79]\tLoss 1.2113 (1.8565)\t\n",
      " Epoch: [1][0/391]\tLoss 0.9945 (0.9945)\t\n",
      " Epoch: [1][200/391]\tLoss 1.1876 (1.2497)\t\n",
      " Epoch: [1][390/391]\tLoss 0.9846 (1.2805)\t\n",
      " Epoch: [1][0/79]\tLoss 1.6766 (1.6766)\t\n",
      " Epoch: [1][78/79]\tLoss 2.1892 (1.8709)\t\n",
      " Epoch: [2][0/391]\tLoss 1.1250 (1.1250)\t\n",
      " Epoch: [2][200/391]\tLoss 1.0516 (1.1148)\t\n",
      " Epoch: [2][390/391]\tLoss 1.0814 (1.1508)\t\n",
      " Epoch: [2][0/79]\tLoss 1.9108 (1.9108)\t\n",
      " Epoch: [2][78/79]\tLoss 1.1043 (1.8455)\t\n",
      " Epoch: [3][0/391]\tLoss 0.9673 (0.9673)\t\n",
      " Epoch: [3][200/391]\tLoss 1.1658 (1.0100)\t\n",
      " Epoch: [3][390/391]\tLoss 1.1948 (1.0442)\t\n",
      " Epoch: [3][0/79]\tLoss 1.9645 (1.9645)\t\n",
      " Epoch: [3][78/79]\tLoss 1.7141 (1.8941)\t\n",
      " Epoch: [4][0/391]\tLoss 0.8583 (0.8583)\t\n",
      " Epoch: [4][200/391]\tLoss 0.9809 (0.9258)\t\n",
      " Epoch: [4][390/391]\tLoss 1.1665 (0.9607)\t\n",
      " Epoch: [4][0/79]\tLoss 1.8195 (1.8195)\t\n",
      " Epoch: [4][78/79]\tLoss 2.6040 (1.9400)\t\n",
      " Epoch: [5][0/391]\tLoss 0.7109 (0.7109)\t\n",
      " Epoch: [5][200/391]\tLoss 0.7449 (0.8125)\t\n",
      " Epoch: [5][390/391]\tLoss 1.0499 (0.8506)\t\n",
      " Epoch: [5][0/79]\tLoss 1.9421 (1.9421)\t\n",
      " Epoch: [5][78/79]\tLoss 1.9116 (1.9630)\t\n",
      " Epoch: [6][0/391]\tLoss 0.5757 (0.5757)\t\n",
      " Epoch: [6][200/391]\tLoss 0.7905 (0.7257)\t\n",
      " Epoch: [6][390/391]\tLoss 0.8066 (0.7751)\t\n",
      " Epoch: [6][0/79]\tLoss 2.0709 (2.0709)\t\n",
      " Epoch: [6][78/79]\tLoss 3.0991 (2.0464)\t\n",
      " Epoch: [7][0/391]\tLoss 0.7349 (0.7349)\t\n",
      " Epoch: [7][200/391]\tLoss 0.8548 (0.6478)\t\n",
      " Epoch: [7][390/391]\tLoss 0.6993 (0.6902)\t\n",
      " Epoch: [7][0/79]\tLoss 2.1701 (2.1701)\t\n",
      " Epoch: [7][78/79]\tLoss 1.8634 (2.1022)\t\n",
      " Epoch: [8][0/391]\tLoss 0.5813 (0.5813)\t\n",
      " Epoch: [8][200/391]\tLoss 0.6146 (0.5919)\t\n",
      " Epoch: [8][390/391]\tLoss 0.6223 (0.6296)\t\n",
      " Epoch: [8][0/79]\tLoss 2.4138 (2.4138)\t\n",
      " Epoch: [8][78/79]\tLoss 2.5827 (2.1739)\t\n",
      " Epoch: [9][0/391]\tLoss 0.5448 (0.5448)\t\n",
      " Epoch: [9][200/391]\tLoss 0.5571 (0.5385)\t\n",
      " Epoch: [9][390/391]\tLoss 0.6290 (0.5690)\t\n",
      " Epoch: [9][0/79]\tLoss 2.0909 (2.0909)\t\n",
      " Epoch: [9][78/79]\tLoss 1.7316 (2.2168)\t\n",
      " Epoch: [10][0/391]\tLoss 0.5791 (0.5791)\t\n",
      " Epoch: [10][200/391]\tLoss 0.5491 (0.4791)\t\n",
      " Epoch: [10][390/391]\tLoss 0.6763 (0.5141)\t\n",
      " Epoch: [10][0/79]\tLoss 2.4072 (2.4072)\t\n",
      " Epoch: [10][78/79]\tLoss 1.3546 (2.2557)\t\n",
      "early stopping.\n",
      "Using augmented CIFAR 100.\n",
      "Files already downloaded and verified\n",
      " Epoch: [0][0/391]\tLoss 1.0761 (1.0761)\t\n",
      " Epoch: [0][200/391]\tLoss 1.6577 (2.2531)\t\n",
      " Epoch: [0][390/391]\tLoss 1.6017 (2.0184)\t\n",
      " Epoch: [0][0/79]\tLoss 2.0262 (2.0262)\t\n",
      " Epoch: [0][78/79]\tLoss 1.6920 (1.9438)\t\n",
      " Epoch: [1][0/391]\tLoss 1.4677 (1.4677)\t\n",
      " Epoch: [1][200/391]\tLoss 1.6099 (1.5892)\t\n",
      " Epoch: [1][390/391]\tLoss 1.7071 (1.5763)\t\n",
      " Epoch: [1][0/79]\tLoss 2.0776 (2.0776)\t\n",
      " Epoch: [1][78/79]\tLoss 2.0966 (1.9095)\t\n",
      " Epoch: [2][0/391]\tLoss 1.2891 (1.2891)\t\n",
      " Epoch: [2][200/391]\tLoss 1.2185 (1.4176)\t\n",
      " Epoch: [2][390/391]\tLoss 1.5307 (1.4344)\t\n",
      " Epoch: [2][0/79]\tLoss 1.6046 (1.6046)\t\n",
      " Epoch: [2][78/79]\tLoss 1.4189 (1.9060)\t\n",
      " Epoch: [3][0/391]\tLoss 1.1533 (1.1533)\t\n",
      " Epoch: [3][200/391]\tLoss 1.5213 (1.2858)\t\n",
      " Epoch: [3][390/391]\tLoss 1.5689 (1.3181)\t\n",
      " Epoch: [3][0/79]\tLoss 2.0250 (2.0250)\t\n",
      " Epoch: [3][78/79]\tLoss 0.7518 (1.8858)\t\n",
      " Epoch: [4][0/391]\tLoss 1.0293 (1.0293)\t\n",
      " Epoch: [4][200/391]\tLoss 1.2095 (1.1903)\t\n",
      " Epoch: [4][390/391]\tLoss 1.1349 (1.2082)\t\n",
      " Epoch: [4][0/79]\tLoss 1.9099 (1.9099)\t\n",
      " Epoch: [4][78/79]\tLoss 2.5328 (1.8652)\t\n",
      " Epoch: [5][0/391]\tLoss 1.1481 (1.1481)\t\n",
      " Epoch: [5][200/391]\tLoss 0.9698 (1.0658)\t\n",
      " Epoch: [5][390/391]\tLoss 1.2565 (1.1029)\t\n",
      " Epoch: [5][0/79]\tLoss 1.9019 (1.9019)\t\n",
      " Epoch: [5][78/79]\tLoss 1.7589 (1.8992)\t\n",
      " Epoch: [6][0/391]\tLoss 0.9633 (0.9633)\t\n",
      " Epoch: [6][200/391]\tLoss 0.9549 (0.9736)\t\n",
      " Epoch: [6][390/391]\tLoss 0.9358 (1.0004)\t\n",
      " Epoch: [6][0/79]\tLoss 1.5788 (1.5788)\t\n",
      " Epoch: [6][78/79]\tLoss 2.4285 (1.9539)\t\n",
      " Epoch: [7][0/391]\tLoss 1.0404 (1.0404)\t\n",
      " Epoch: [7][200/391]\tLoss 0.8970 (0.8846)\t\n",
      " Epoch: [7][390/391]\tLoss 1.0645 (0.9280)\t\n",
      " Epoch: [7][0/79]\tLoss 1.6642 (1.6642)\t\n",
      " Epoch: [7][78/79]\tLoss 1.8035 (1.9821)\t\n",
      " Epoch: [8][0/391]\tLoss 0.5987 (0.5987)\t\n",
      " Epoch: [8][200/391]\tLoss 0.7310 (0.8132)\t\n",
      " Epoch: [8][390/391]\tLoss 1.0616 (0.8419)\t\n",
      " Epoch: [8][0/79]\tLoss 2.3468 (2.3468)\t\n",
      " Epoch: [8][78/79]\tLoss 0.6271 (2.0097)\t\n",
      " Epoch: [9][0/391]\tLoss 0.5455 (0.5455)\t\n",
      " Epoch: [9][200/391]\tLoss 0.8919 (0.7253)\t\n",
      " Epoch: [9][390/391]\tLoss 0.8915 (0.7719)\t\n",
      " Epoch: [9][0/79]\tLoss 2.2167 (2.2167)\t\n",
      " Epoch: [9][78/79]\tLoss 1.0373 (2.0867)\t\n",
      " Epoch: [10][0/391]\tLoss 0.6287 (0.6287)\t\n",
      " Epoch: [10][200/391]\tLoss 0.9644 (0.6738)\t\n",
      " Epoch: [10][390/391]\tLoss 1.0639 (0.7080)\t\n",
      " Epoch: [10][0/79]\tLoss 2.3418 (2.3418)\t\n",
      " Epoch: [10][78/79]\tLoss 1.6756 (2.1619)\t\n",
      " Epoch: [11][0/391]\tLoss 0.4771 (0.4771)\t\n",
      " Epoch: [11][200/391]\tLoss 0.6847 (0.6095)\t\n",
      " Epoch: [11][390/391]\tLoss 1.0095 (0.6422)\t\n",
      " Epoch: [11][0/79]\tLoss 2.0966 (2.0966)\t\n",
      " Epoch: [11][78/79]\tLoss 1.5149 (2.1418)\t\n",
      " Epoch: [12][0/391]\tLoss 0.4220 (0.4220)\t\n",
      " Epoch: [12][200/391]\tLoss 0.5193 (0.5639)\t\n",
      " Epoch: [12][390/391]\tLoss 0.5952 (0.5931)\t\n",
      " Epoch: [12][0/79]\tLoss 2.4707 (2.4707)\t\n",
      " Epoch: [12][78/79]\tLoss 1.6361 (2.2107)\t\n",
      "early stopping.\n",
      "Using augmented CIFAR 100.\n",
      "Files already downloaded and verified\n",
      " Epoch: [0][0/391]\tLoss 1.4106 (1.4106)\t\n",
      " Epoch: [0][200/391]\tLoss 3.1403 (3.7624)\t\n",
      " Epoch: [0][390/391]\tLoss 2.6843 (3.3508)\t\n",
      " Epoch: [0][0/79]\tLoss 2.8257 (2.8257)\t\n",
      " Epoch: [0][78/79]\tLoss 2.9156 (2.7153)\t\n",
      " Epoch: [1][0/391]\tLoss 2.6119 (2.6119)\t\n",
      " Epoch: [1][200/391]\tLoss 2.4791 (2.4792)\t\n",
      " Epoch: [1][390/391]\tLoss 2.2715 (2.3985)\t\n",
      " Epoch: [1][0/79]\tLoss 2.2540 (2.2540)\t\n",
      " Epoch: [1][78/79]\tLoss 2.7246 (2.2417)\t\n",
      " Epoch: [2][0/391]\tLoss 1.9897 (1.9897)\t\n",
      " Epoch: [2][200/391]\tLoss 2.1363 (2.1020)\t\n",
      " Epoch: [2][390/391]\tLoss 2.4140 (2.0679)\t\n",
      " Epoch: [2][0/79]\tLoss 1.7359 (1.7359)\t\n",
      " Epoch: [2][78/79]\tLoss 1.8561 (2.0678)\t\n",
      " Epoch: [3][0/391]\tLoss 1.7735 (1.7735)\t\n",
      " Epoch: [3][200/391]\tLoss 1.8780 (1.8773)\t\n",
      " Epoch: [3][390/391]\tLoss 2.1276 (1.8678)\t\n",
      " Epoch: [3][0/79]\tLoss 2.0332 (2.0332)\t\n",
      " Epoch: [3][78/79]\tLoss 2.8495 (2.0241)\t\n",
      " Epoch: [4][0/391]\tLoss 1.7925 (1.7925)\t\n",
      " Epoch: [4][200/391]\tLoss 1.7156 (1.7368)\t\n",
      " Epoch: [6][390/391]\tLoss 1.6772 (1.5088)\t\n",
      " Epoch: [6][0/79]\tLoss 1.8338 (1.8338)\t\n",
      " Epoch: [6][78/79]\tLoss 1.7284 (1.9370)\t\n",
      " Epoch: [7][0/391]\tLoss 1.3367 (1.3367)\t\n",
      " Epoch: [7][200/391]\tLoss 1.2956 (1.3842)\t\n",
      " Epoch: [7][390/391]\tLoss 1.6330 (1.4054)\t\n",
      " Epoch: [7][0/79]\tLoss 2.0256 (2.0256)\t\n",
      " Epoch: [7][78/79]\tLoss 2.1062 (1.9390)\t\n",
      " Epoch: [8][0/391]\tLoss 1.3572 (1.3572)\t\n",
      " Epoch: [8][200/391]\tLoss 1.1735 (1.2896)\t\n",
      " Epoch: [8][390/391]\tLoss 1.1673 (1.3122)\t\n",
      " Epoch: [8][0/79]\tLoss 1.8502 (1.8502)\t\n",
      " Epoch: [8][78/79]\tLoss 1.9581 (1.9387)\t\n",
      " Epoch: [9][0/391]\tLoss 1.1492 (1.1492)\t\n",
      " Epoch: [9][200/391]\tLoss 0.8720 (1.1963)\t\n",
      " Epoch: [9][390/391]\tLoss 1.0404 (1.2282)\t\n",
      " Epoch: [9][0/79]\tLoss 1.6271 (1.6271)\t\n",
      " Epoch: [9][78/79]\tLoss 1.9893 (1.9359)\t\n",
      " Epoch: [10][0/391]\tLoss 1.0176 (1.0176)\t\n",
      " Epoch: [10][200/391]\tLoss 1.2918 (1.1012)\t\n",
      " Epoch: [10][390/391]\tLoss 1.2828 (1.1403)\t\n",
      " Epoch: [10][0/79]\tLoss 1.9616 (1.9616)\t\n",
      " Epoch: [10][78/79]\tLoss 1.5612 (1.9316)\t\n",
      " Epoch: [11][0/391]\tLoss 1.0320 (1.0320)\t\n",
      " Epoch: [11][200/391]\tLoss 1.0702 (1.0060)\t\n",
      " Epoch: [11][390/391]\tLoss 0.9638 (1.0578)\t\n",
      " Epoch: [11][0/79]\tLoss 2.0002 (2.0002)\t\n",
      " Epoch: [11][78/79]\tLoss 3.1044 (2.0067)\t\n",
      " Epoch: [12][0/391]\tLoss 0.8189 (0.8189)\t\n",
      " Epoch: [12][200/391]\tLoss 0.9549 (0.9358)\t\n",
      " Epoch: [12][390/391]\tLoss 0.9251 (0.9799)\t\n",
      " Epoch: [12][0/79]\tLoss 1.9001 (1.9001)\t\n",
      " Epoch: [12][78/79]\tLoss 2.9352 (2.0193)\t\n",
      " Epoch: [13][0/391]\tLoss 0.7257 (0.7257)\t\n",
      " Epoch: [13][200/391]\tLoss 1.0454 (0.8747)\t\n",
      " Epoch: [13][390/391]\tLoss 0.7579 (0.9095)\t\n",
      " Epoch: [13][0/79]\tLoss 1.9852 (1.9852)\t\n",
      " Epoch: [13][78/79]\tLoss 2.2450 (2.0451)\t\n",
      " Epoch: [14][0/391]\tLoss 0.7375 (0.7375)\t\n",
      " Epoch: [14][200/391]\tLoss 1.0387 (0.8034)\t\n",
      " Epoch: [14][390/391]\tLoss 1.0551 (0.8426)\t\n",
      " Epoch: [14][0/79]\tLoss 2.0679 (2.0679)\t\n",
      " Epoch: [14][78/79]\tLoss 2.2846 (2.1001)\t\n",
      " Epoch: [15][0/391]\tLoss 0.5367 (0.5367)\t\n",
      " Epoch: [15][200/391]\tLoss 0.6428 (0.7463)\t\n",
      " Epoch: [15][390/391]\tLoss 0.8507 (0.7848)\t\n",
      " Epoch: [15][0/79]\tLoss 2.1054 (2.1054)\t\n",
      " Epoch: [15][78/79]\tLoss 2.5628 (2.1420)\t\n",
      " Epoch: [16][0/391]\tLoss 0.5430 (0.5430)\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: [16][200/391]\tLoss 0.8422 (0.6801)\t\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'device' : 'cuda:5',\n",
    "    'net_name' : 'vit_B_16_imagenet1k',\n",
    "    'dataset_name' : 'cifar100',\n",
    "    'dataset_args': {\n",
    "        'batch_size' : 128,\n",
    "    },\n",
    "    'early_stopping_step' : 7,\n",
    "    'lr' : 0.0005,\n",
    "    'model_args' : {\n",
    "        'num_classes' : 100\n",
    "        'image_size' : 32,\n",
    "        'in_channels' : 3,\n",
    "    },\n",
    "    'model_name' : 'B_16_imagenet1k',\n",
    "}\n",
    "\n",
    "for ratio in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "    args['reg_ratio']  = ratio\n",
    "    main(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{python}\n",
    "device = 'cuda:5'\n",
    "net_name = 'vit'\n",
    "dataset_name = 'cifar10'\n",
    "batch_size = 128\n",
    "early_stopping_step = 5\n",
    "lr = 0.0005\n",
    "\n",
    "model_args = {\n",
    "    'num_classes' : 10,\n",
    "    'image_size' : 32,\n",
    "    'in_channels' : 3    ,\n",
    "}\n",
    "model_name = 'B_16_imagenet1k'\n",
    "\n",
    "net = ViT(model_name, True, **model_args)\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "trainset, testset, num_classes = cifar10(batch_size=batch_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ViT Model\n",
    "```{python}\n",
    "\n",
    "best_state = None\n",
    "best_loss = 9999999\n",
    "early_stopping_idx = 0\n",
    "for epoch in range(100):\n",
    "    early_stopping_idx += 1\n",
    "    train_state = train_iter(net, trainset, criterion, optimizer, epoch, device)\n",
    "    valid_state = valid_iter(net, testset, criterion, optimizer, epoch, device)\n",
    "    \n",
    "    if best_loss > valid_state['loss']:\n",
    "        best_loss = valid_state['loss']\n",
    "        best_state = train_state\n",
    "        \n",
    "        early_stopping_idx = 0\n",
    "        \n",
    "    if early_stopping_idx > early_stopping_step:\n",
    "        print('early stopping.')\n",
    "        break\n",
    "        \n",
    "if not os.path.exists('{}/{}_{}/'.format(BASE_PATH, dataset_name, net_name)):\n",
    "    os.mkdir('{}/{}_{}/'.format(BASE_PATH, dataset_name, net_name))\n",
    "\n",
    "torch.save(best_state, '{}/{}_{}/best_state.ptl'.format(BASE_PATH, dataset_name, net_name))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmnist, mnist, imagenet, cifar10, cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized positional embeddings from torch.Size([1, 577, 768]) to torch.Size([1, 2, 768])\n",
      "Loaded pretrained weights.\n",
      " Epoch: [0][0/469]\tLoss 2.3026 (2.3026)\t\n",
      " Epoch: [0][200/469]\tLoss 0.8916 (1.1547)\t\n",
      " Epoch: [0][400/469]\tLoss 0.7908 (0.9405)\t\n",
      " Epoch: [0][0/79]\tLoss 0.5761 (0.5761)\t\n",
      " Epoch: [1][0/469]\tLoss 0.8311 (0.8311)\t\n",
      " Epoch: [1][200/469]\tLoss 0.6708 (0.6294)\t\n",
      " Epoch: [1][400/469]\tLoss 0.7531 (0.6185)\t\n",
      " Epoch: [1][0/79]\tLoss 0.5679 (0.5679)\t\n",
      " Epoch: [2][0/469]\tLoss 0.5744 (0.5744)\t\n",
      " Epoch: [2][200/469]\tLoss 0.5523 (0.5655)\t\n",
      " Epoch: [2][400/469]\tLoss 0.6629 (0.5607)\t\n",
      " Epoch: [2][0/79]\tLoss 0.5592 (0.5592)\t\n",
      " Epoch: [3][0/469]\tLoss 0.3930 (0.3930)\t\n",
      " Epoch: [3][200/469]\tLoss 0.6584 (0.5280)\t\n",
      " Epoch: [3][400/469]\tLoss 0.4988 (0.5261)\t\n",
      " Epoch: [3][0/79]\tLoss 0.6163 (0.6163)\t\n",
      " Epoch: [4][0/469]\tLoss 0.5425 (0.5425)\t\n",
      " Epoch: [4][200/469]\tLoss 0.6367 (0.5121)\t\n",
      " Epoch: [4][400/469]\tLoss 0.4724 (0.5075)\t\n",
      " Epoch: [4][0/79]\tLoss 0.4423 (0.4423)\t\n",
      " Epoch: [5][0/469]\tLoss 0.4794 (0.4794)\t\n",
      " Epoch: [5][200/469]\tLoss 0.5299 (0.4875)\t\n",
      " Epoch: [5][400/469]\tLoss 0.3519 (0.4839)\t\n",
      " Epoch: [5][0/79]\tLoss 0.4993 (0.4993)\t\n",
      " Epoch: [6][0/469]\tLoss 0.5094 (0.5094)\t\n",
      " Epoch: [6][200/469]\tLoss 0.4398 (0.4679)\t\n",
      " Epoch: [6][400/469]\tLoss 0.4815 (0.4645)\t\n",
      " Epoch: [6][0/79]\tLoss 0.5441 (0.5441)\t\n",
      " Epoch: [7][0/469]\tLoss 0.4498 (0.4498)\t\n",
      " Epoch: [7][200/469]\tLoss 0.3825 (0.4589)\t\n",
      " Epoch: [7][400/469]\tLoss 0.4548 (0.4564)\t\n",
      " Epoch: [7][0/79]\tLoss 0.4536 (0.4536)\t\n",
      " Epoch: [8][0/469]\tLoss 0.5934 (0.5934)\t\n",
      " Epoch: [8][200/469]\tLoss 0.4823 (0.4401)\t\n",
      " Epoch: [8][400/469]\tLoss 0.3610 (0.4417)\t\n",
      " Epoch: [8][0/79]\tLoss 0.5465 (0.5465)\t\n",
      " Epoch: [9][0/469]\tLoss 0.3784 (0.3784)\t\n",
      " Epoch: [9][200/469]\tLoss 0.3223 (0.4261)\t\n",
      " Epoch: [9][400/469]\tLoss 0.4365 (0.4265)\t\n",
      " Epoch: [9][0/79]\tLoss 0.5482 (0.5482)\t\n",
      " Epoch: [10][0/469]\tLoss 0.3409 (0.3409)\t\n",
      " Epoch: [10][200/469]\tLoss 0.3365 (0.4158)\t\n",
      " Epoch: [10][400/469]\tLoss 0.4955 (0.4174)\t\n",
      " Epoch: [10][0/79]\tLoss 0.3865 (0.3865)\t\n",
      " Epoch: [11][0/469]\tLoss 0.3139 (0.3139)\t\n",
      " Epoch: [11][200/469]\tLoss 0.3809 (0.3948)\t\n",
      " Epoch: [11][400/469]\tLoss 0.4538 (0.4055)\t\n",
      " Epoch: [11][0/79]\tLoss 0.4481 (0.4481)\t\n",
      " Epoch: [12][0/469]\tLoss 0.3351 (0.3351)\t\n",
      " Epoch: [12][200/469]\tLoss 0.3986 (0.3932)\t\n",
      " Epoch: [12][400/469]\tLoss 0.3191 (0.3922)\t\n",
      " Epoch: [12][0/79]\tLoss 0.4363 (0.4363)\t\n",
      " Epoch: [13][0/469]\tLoss 0.4977 (0.4977)\t\n",
      " Epoch: [13][200/469]\tLoss 0.3385 (0.3830)\t\n",
      " Epoch: [13][400/469]\tLoss 0.4403 (0.3830)\t\n",
      " Epoch: [13][0/79]\tLoss 0.3977 (0.3977)\t\n",
      " Epoch: [14][0/469]\tLoss 0.3058 (0.3058)\t\n",
      " Epoch: [14][200/469]\tLoss 0.2683 (0.3601)\t\n",
      " Epoch: [14][400/469]\tLoss 0.3404 (0.3677)\t\n",
      " Epoch: [14][0/79]\tLoss 0.5376 (0.5376)\t\n",
      " Epoch: [15][0/469]\tLoss 0.3422 (0.3422)\t\n",
      " Epoch: [15][200/469]\tLoss 0.3016 (0.3572)\t\n",
      " Epoch: [15][400/469]\tLoss 0.2807 (0.3602)\t\n",
      " Epoch: [15][0/79]\tLoss 0.4575 (0.4575)\t\n",
      " Epoch: [16][0/469]\tLoss 0.4264 (0.4264)\t\n",
      " Epoch: [16][200/469]\tLoss 0.3362 (0.3498)\t\n",
      " Epoch: [16][400/469]\tLoss 0.3238 (0.3512)\t\n",
      " Epoch: [16][0/79]\tLoss 0.7336 (0.7336)\t\n",
      " Epoch: [17][0/469]\tLoss 0.3707 (0.3707)\t\n",
      " Epoch: [17][200/469]\tLoss 0.2700 (0.3369)\t\n",
      " Epoch: [17][400/469]\tLoss 0.3276 (0.3389)\t\n",
      " Epoch: [17][0/79]\tLoss 0.6757 (0.6757)\t\n",
      " Epoch: [18][0/469]\tLoss 0.2428 (0.2428)\t\n",
      " Epoch: [18][200/469]\tLoss 0.3548 (0.3291)\t\n",
      " Epoch: [18][400/469]\tLoss 0.2967 (0.3266)\t\n",
      " Epoch: [18][0/79]\tLoss 0.5789 (0.5789)\t\n",
      " Epoch: [19][0/469]\tLoss 0.3453 (0.3453)\t\n",
      " Epoch: [19][200/469]\tLoss 0.3153 (0.3163)\t\n",
      " Epoch: [19][400/469]\tLoss 0.4393 (0.3207)\t\n",
      " Epoch: [19][0/79]\tLoss 0.4238 (0.4238)\t\n",
      "early stopping.\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'device' : 'cuda:5',\n",
    "    'net_name' : 'vit_B_16_imagenet1k',\n",
    "    'dataset_name' : 'fmnist',\n",
    "    'dataset_args': {\n",
    "        'batch_size' : 128,\n",
    "    },\n",
    "    'early_stopping_step' : 7,\n",
    "    'lr' : 0.0005,\n",
    "    'model_args' : {\n",
    "        'num_classes' : 10,\n",
    "        'image_size' : 28,\n",
    "        'in_channels' : 1,\n",
    "    },\n",
    "    'model_name' : 'B_16_imagenet1k',\n",
    "}\n",
    "\n",
    "main(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'device' : 'cuda:5',\n",
    "    'net_name' : 'vit_B_16_imagenet1k',\n",
    "    'dataset_name' : 'fmnist',\n",
    "    'dataset_args': {\n",
    "        'batch_size' : 128,\n",
    "    },\n",
    "    'early_stopping_step' : 7,\n",
    "    'lr' : 0.0005,\n",
    "    'model_args' : {\n",
    "        'num_classes' : 10,\n",
    "        'image_size' : 28,\n",
    "        'in_channels' : 1,\n",
    "    },\n",
    "    'model_name' : 'B_16_imagenet1k',\n",
    "}\n",
    "\n",
    "for ratio in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "    args['reg_ratio']  = ratio\n",
    "    main(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized positional embeddings from torch.Size([1, 577, 768]) to torch.Size([1, 2, 768])\n",
      "Loaded pretrained weights.\n",
      " Epoch: [0][0/469]\tLoss 2.3026 (2.3026)\t\n",
      " Epoch: [0][200/469]\tLoss 0.6219 (1.0719)\t\n",
      " Epoch: [0][400/469]\tLoss 0.5050 (0.8242)\t\n",
      " Epoch: [0][0/79]\tLoss 0.3814 (0.3814)\t\n",
      " Epoch: [1][0/469]\tLoss 0.4314 (0.4314)\t\n",
      " Epoch: [1][200/469]\tLoss 0.4443 (0.4555)\t\n",
      " Epoch: [1][400/469]\tLoss 0.5112 (0.4329)\t\n",
      " Epoch: [1][0/79]\tLoss 0.3165 (0.3165)\t\n",
      " Epoch: [2][0/469]\tLoss 0.2767 (0.2767)\t\n",
      " Epoch: [2][200/469]\tLoss 0.3829 (0.3612)\t\n",
      " Epoch: [2][400/469]\tLoss 0.3456 (0.3600)\t\n",
      " Epoch: [2][0/79]\tLoss 0.4342 (0.4342)\t\n",
      " Epoch: [3][0/469]\tLoss 0.2976 (0.2976)\t\n",
      " Epoch: [3][200/469]\tLoss 0.2563 (0.3286)\t\n",
      " Epoch: [3][400/469]\tLoss 0.3472 (0.3260)\t\n",
      " Epoch: [3][0/79]\tLoss 0.3588 (0.3588)\t\n",
      " Epoch: [4][0/469]\tLoss 0.4007 (0.4007)\t\n",
      " Epoch: [4][200/469]\tLoss 0.2141 (0.2958)\t\n",
      " Epoch: [4][400/469]\tLoss 0.3258 (0.2963)\t\n",
      " Epoch: [4][0/79]\tLoss 0.2515 (0.2515)\t\n",
      " Epoch: [5][0/469]\tLoss 0.1945 (0.1945)\t\n",
      " Epoch: [5][200/469]\tLoss 0.3169 (0.2755)\t\n",
      " Epoch: [5][400/469]\tLoss 0.2320 (0.2762)\t\n",
      " Epoch: [5][0/79]\tLoss 0.2920 (0.2920)\t\n",
      " Epoch: [6][0/469]\tLoss 0.1916 (0.1916)\t\n",
      " Epoch: [6][200/469]\tLoss 0.2278 (0.2492)\t\n",
      " Epoch: [6][400/469]\tLoss 0.2536 (0.2598)\t\n",
      " Epoch: [6][0/79]\tLoss 0.3228 (0.3228)\t\n",
      " Epoch: [7][0/469]\tLoss 0.1525 (0.1525)\t\n",
      " Epoch: [7][200/469]\tLoss 0.2071 (0.2423)\t\n",
      " Epoch: [7][400/469]\tLoss 0.1992 (0.2440)\t\n",
      " Epoch: [7][0/79]\tLoss 0.2706 (0.2706)\t\n",
      " Epoch: [8][0/469]\tLoss 0.2984 (0.2984)\t\n",
      " Epoch: [8][200/469]\tLoss 0.2431 (0.2356)\t\n",
      " Epoch: [8][400/469]\tLoss 0.1337 (0.2326)\t\n",
      " Epoch: [8][0/79]\tLoss 0.3453 (0.3453)\t\n",
      " Epoch: [9][0/469]\tLoss 0.2037 (0.2037)\t\n",
      " Epoch: [9][200/469]\tLoss 0.2184 (0.2083)\t\n",
      " Epoch: [9][400/469]\tLoss 0.1629 (0.2161)\t\n",
      " Epoch: [9][0/79]\tLoss 0.2194 (0.2194)\t\n",
      " Epoch: [10][0/469]\tLoss 0.2314 (0.2314)\t\n",
      " Epoch: [10][200/469]\tLoss 0.1675 (0.2007)\t\n",
      " Epoch: [10][400/469]\tLoss 0.2025 (0.2059)\t\n",
      " Epoch: [10][0/79]\tLoss 0.1273 (0.1273)\t\n",
      " Epoch: [11][0/469]\tLoss 0.1786 (0.1786)\t\n",
      " Epoch: [11][200/469]\tLoss 0.1896 (0.1974)\t\n",
      " Epoch: [11][400/469]\tLoss 0.2228 (0.1969)\t\n",
      " Epoch: [11][0/79]\tLoss 0.2773 (0.2773)\t\n",
      " Epoch: [12][0/469]\tLoss 0.1207 (0.1207)\t\n",
      " Epoch: [12][200/469]\tLoss 0.2985 (0.1820)\t\n",
      " Epoch: [12][400/469]\tLoss 0.2390 (0.1859)\t\n",
      " Epoch: [12][0/79]\tLoss 0.2785 (0.2785)\t\n",
      " Epoch: [13][0/469]\tLoss 0.1062 (0.1062)\t\n",
      " Epoch: [13][200/469]\tLoss 0.0796 (0.1729)\t\n",
      " Epoch: [13][400/469]\tLoss 0.1105 (0.1764)\t\n",
      " Epoch: [13][0/79]\tLoss 0.2935 (0.2935)\t\n",
      " Epoch: [14][0/469]\tLoss 0.1144 (0.1144)\t\n",
      " Epoch: [14][200/469]\tLoss 0.1417 (0.1634)\t\n",
      " Epoch: [14][400/469]\tLoss 0.1346 (0.1692)\t\n",
      " Epoch: [14][0/79]\tLoss 0.2164 (0.2164)\t\n",
      " Epoch: [15][0/469]\tLoss 0.1520 (0.1520)\t\n",
      " Epoch: [15][200/469]\tLoss 0.1235 (0.1466)\t\n",
      " Epoch: [15][400/469]\tLoss 0.2394 (0.1588)\t\n",
      " Epoch: [15][0/79]\tLoss 0.2504 (0.2504)\t\n",
      " Epoch: [16][0/469]\tLoss 0.1755 (0.1755)\t\n",
      " Epoch: [16][200/469]\tLoss 0.0764 (0.1454)\t\n",
      " Epoch: [16][400/469]\tLoss 0.1607 (0.1520)\t\n",
      " Epoch: [16][0/79]\tLoss 0.2493 (0.2493)\t\n",
      " Epoch: [17][0/469]\tLoss 0.1623 (0.1623)\t\n",
      " Epoch: [17][200/469]\tLoss 0.1472 (0.1378)\t\n",
      " Epoch: [17][400/469]\tLoss 0.1356 (0.1470)\t\n",
      " Epoch: [17][0/79]\tLoss 0.2507 (0.2507)\t\n",
      " Epoch: [18][0/469]\tLoss 0.1517 (0.1517)\t\n",
      " Epoch: [18][200/469]\tLoss 0.1627 (0.1368)\t\n",
      " Epoch: [18][400/469]\tLoss 0.1061 (0.1393)\t\n",
      " Epoch: [18][0/79]\tLoss 0.4131 (0.4131)\t\n",
      " Epoch: [19][0/469]\tLoss 0.0819 (0.0819)\t\n",
      " Epoch: [19][200/469]\tLoss 0.1206 (0.1308)\t\n",
      " Epoch: [19][400/469]\tLoss 0.1210 (0.1312)\t\n",
      " Epoch: [19][0/79]\tLoss 0.2880 (0.2880)\t\n",
      "early stopping.\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'device' : 'cuda:5',\n",
    "    'net_name' : 'vit_B_16_imagenet1k',\n",
    "    'dataset_name' : 'mnist',\n",
    "    'dataset_args': {\n",
    "        'batch_size' : 128,\n",
    "    },\n",
    "    'early_stopping_step' : 7,\n",
    "    'lr' : 0.0005,\n",
    "    'model_args' : {\n",
    "        'num_classes' : 10,\n",
    "        'image_size' : 28,\n",
    "        'in_channels' : 1,\n",
    "    },\n",
    "    'model_name' : 'B_16_imagenet1k',\n",
    "}\n",
    "\n",
    "main(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'device' : 'cuda:5',\n",
    "    'net_name' : 'vit_B_16_imagenet1k',\n",
    "    'dataset_name' : 'mnist',\n",
    "    'dataset_args': {\n",
    "        'batch_size' : 128,\n",
    "    },\n",
    "    'early_stopping_step' : 7,\n",
    "    'lr' : 0.0005,\n",
    "    'model_args' : {\n",
    "        'num_classes' : 10,\n",
    "        'image_size' : 28,\n",
    "        'in_channels' : 1,\n",
    "    },\n",
    "    'model_name' : 'B_16_imagenet1k',\n",
    "}\n",
    "\n",
    "for ratio in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "    args['reg_ratio']  = ratio\n",
    "    main(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized positional embeddings from torch.Size([1, 577, 768]) to torch.Size([1, 197, 768])\n",
      "Loaded pretrained weights.\n",
      "Using augmented IMAGENET.\n",
      " Epoch: [0][0/813]\tLoss 2.3026 (2.3026)\t\n",
      " Epoch: [0][200/813]\tLoss 1.1444 (1.3426)\t\n",
      " Epoch: [0][400/813]\tLoss 1.0616 (1.2252)\t\n",
      " Epoch: [0][600/813]\tLoss 0.8956 (1.1705)\t\n",
      " Epoch: [0][800/813]\tLoss 1.6084 (1.1251)\t\n",
      " Epoch: [0][0/32]\tLoss 1.3876 (1.3876)\t\n",
      " Epoch: [1][0/813]\tLoss 0.4881 (0.4881)\t\n",
      " Epoch: [1][200/813]\tLoss 0.6999 (0.9505)\t\n",
      " Epoch: [1][400/813]\tLoss 0.9489 (0.9373)\t\n",
      " Epoch: [1][600/813]\tLoss 1.0469 (0.9334)\t\n",
      " Epoch: [1][800/813]\tLoss 2.1118 (0.9313)\t\n",
      " Epoch: [1][0/32]\tLoss 0.6431 (0.6431)\t\n",
      " Epoch: [2][0/813]\tLoss 1.0754 (1.0754)\t\n",
      " Epoch: [2][200/813]\tLoss 0.8215 (0.8467)\t\n",
      " Epoch: [2][400/813]\tLoss 0.6404 (0.8831)\t\n",
      " Epoch: [2][600/813]\tLoss 0.6288 (0.8815)\t\n",
      " Epoch: [2][800/813]\tLoss 1.2450 (0.8779)\t\n",
      " Epoch: [2][0/32]\tLoss 0.5252 (0.5252)\t\n",
      " Epoch: [3][0/813]\tLoss 0.9270 (0.9270)\t\n",
      " Epoch: [3][200/813]\tLoss 0.4926 (0.8811)\t\n",
      " Epoch: [3][400/813]\tLoss 0.3342 (0.8737)\t\n",
      " Epoch: [3][600/813]\tLoss 0.7834 (0.8714)\t\n",
      " Epoch: [3][800/813]\tLoss 0.9097 (0.8672)\t\n",
      " Epoch: [3][0/32]\tLoss 0.9443 (0.9443)\t\n",
      " Epoch: [4][0/813]\tLoss 0.8748 (0.8748)\t\n",
      " Epoch: [4][200/813]\tLoss 0.5847 (0.8150)\t\n",
      " Epoch: [4][400/813]\tLoss 1.0327 (0.8368)\t\n",
      " Epoch: [4][600/813]\tLoss 1.4063 (0.8375)\t\n",
      " Epoch: [4][800/813]\tLoss 1.3914 (0.8384)\t\n",
      " Epoch: [4][0/32]\tLoss 0.9347 (0.9347)\t\n",
      " Epoch: [5][0/813]\tLoss 0.9252 (0.9252)\t\n",
      " Epoch: [5][200/813]\tLoss 1.1590 (0.8253)\t\n",
      " Epoch: [5][400/813]\tLoss 0.8249 (0.8368)\t\n",
      " Epoch: [5][600/813]\tLoss 0.5657 (0.8375)\t\n",
      " Epoch: [5][800/813]\tLoss 0.6642 (0.8318)\t\n",
      " Epoch: [5][0/32]\tLoss 0.5914 (0.5914)\t\n",
      " Epoch: [6][0/813]\tLoss 0.9050 (0.9050)\t\n",
      " Epoch: [6][200/813]\tLoss 0.5944 (0.8120)\t\n",
      " Epoch: [6][400/813]\tLoss 1.1263 (0.8092)\t\n",
      " Epoch: [6][600/813]\tLoss 0.7902 (0.8160)\t\n",
      " Epoch: [6][800/813]\tLoss 0.6602 (0.8198)\t\n",
      " Epoch: [6][0/32]\tLoss 1.0598 (1.0598)\t\n",
      " Epoch: [7][0/813]\tLoss 1.0838 (1.0838)\t\n",
      " Epoch: [7][200/813]\tLoss 0.8514 (0.7719)\t\n",
      " Epoch: [7][400/813]\tLoss 0.7897 (0.7834)\t\n",
      " Epoch: [7][600/813]\tLoss 0.5728 (0.7901)\t\n",
      " Epoch: [7][800/813]\tLoss 0.7219 (0.7916)\t\n",
      " Epoch: [7][0/32]\tLoss 1.1256 (1.1256)\t\n",
      " Epoch: [8][0/813]\tLoss 0.9733 (0.9733)\t\n",
      " Epoch: [8][200/813]\tLoss 0.9229 (0.7694)\t\n",
      " Epoch: [8][400/813]\tLoss 0.6707 (0.7699)\t\n",
      " Epoch: [8][600/813]\tLoss 1.1679 (0.7736)\t\n",
      " Epoch: [8][800/813]\tLoss 1.0057 (0.7821)\t\n",
      " Epoch: [8][0/32]\tLoss 0.6673 (0.6673)\t\n",
      " Epoch: [9][0/813]\tLoss 0.4380 (0.4380)\t\n",
      " Epoch: [9][200/813]\tLoss 0.5873 (0.7705)\t\n",
      " Epoch: [9][400/813]\tLoss 0.7775 (0.7717)\t\n",
      " Epoch: [9][600/813]\tLoss 0.2676 (0.7733)\t\n",
      " Epoch: [9][800/813]\tLoss 0.9093 (0.7774)\t\n",
      " Epoch: [9][0/32]\tLoss 0.8267 (0.8267)\t\n",
      " Epoch: [10][0/813]\tLoss 0.8316 (0.8316)\t\n",
      " Epoch: [10][200/813]\tLoss 0.3253 (0.7513)\t\n",
      " Epoch: [10][400/813]\tLoss 0.9160 (0.7385)\t\n",
      " Epoch: [10][600/813]\tLoss 0.3603 (0.7517)\t\n",
      " Epoch: [10][800/813]\tLoss 0.5738 (0.7524)\t\n",
      " Epoch: [10][0/32]\tLoss 0.6651 (0.6651)\t\n",
      " Epoch: [11][0/813]\tLoss 0.6732 (0.6732)\t\n",
      " Epoch: [11][200/813]\tLoss 0.5501 (0.7260)\t\n",
      " Epoch: [11][400/813]\tLoss 1.0880 (0.7222)\t\n",
      " Epoch: [11][600/813]\tLoss 0.9909 (0.7270)\t\n",
      " Epoch: [11][800/813]\tLoss 0.7223 (0.7360)\t\n",
      " Epoch: [11][0/32]\tLoss 1.0885 (1.0885)\t\n",
      " Epoch: [12][0/813]\tLoss 0.8659 (0.8659)\t\n",
      " Epoch: [12][200/813]\tLoss 0.9095 (0.7109)\t\n",
      " Epoch: [12][400/813]\tLoss 0.6961 (0.7059)\t\n",
      " Epoch: [12][600/813]\tLoss 0.6737 (0.7206)\t\n",
      " Epoch: [12][800/813]\tLoss 0.5022 (0.7269)\t\n",
      " Epoch: [12][0/32]\tLoss 0.3952 (0.3952)\t\n",
      " Epoch: [13][0/813]\tLoss 0.9918 (0.9918)\t\n",
      " Epoch: [13][200/813]\tLoss 0.6266 (0.7223)\t\n",
      " Epoch: [13][400/813]\tLoss 0.4314 (0.7229)\t\n",
      " Epoch: [13][600/813]\tLoss 0.6679 (0.7099)\t\n",
      " Epoch: [13][800/813]\tLoss 0.5342 (0.7211)\t\n",
      " Epoch: [13][0/32]\tLoss 0.8204 (0.8204)\t\n",
      " Epoch: [14][0/813]\tLoss 0.4332 (0.4332)\t\n",
      " Epoch: [14][200/813]\tLoss 0.4656 (0.6911)\t\n",
      " Epoch: [14][400/813]\tLoss 1.0565 (0.6926)\t\n",
      " Epoch: [14][600/813]\tLoss 0.3712 (0.6959)\t\n",
      " Epoch: [14][800/813]\tLoss 0.8789 (0.7025)\t\n",
      " Epoch: [14][0/32]\tLoss 1.5376 (1.5376)\t\n",
      " Epoch: [15][0/813]\tLoss 0.5791 (0.5791)\t\n",
      " Epoch: [15][200/813]\tLoss 0.5699 (0.6749)\t\n",
      " Epoch: [15][400/813]\tLoss 1.2839 (0.6907)\t\n",
      " Epoch: [15][600/813]\tLoss 0.9914 (0.7007)\t\n",
      " Epoch: [15][800/813]\tLoss 0.5283 (0.6954)\t\n",
      " Epoch: [15][0/32]\tLoss 0.3236 (0.3236)\t\n",
      " Epoch: [16][0/813]\tLoss 0.6190 (0.6190)\t\n",
      " Epoch: [16][200/813]\tLoss 0.6007 (0.6713)\t\n",
      " Epoch: [16][400/813]\tLoss 0.4271 (0.6633)\t\n",
      " Epoch: [16][600/813]\tLoss 0.5491 (0.6770)\t\n",
      " Epoch: [16][800/813]\tLoss 0.3098 (0.6762)\t\n",
      " Epoch: [16][0/32]\tLoss 0.3976 (0.3976)\t\n",
      " Epoch: [17][0/813]\tLoss 0.7360 (0.7360)\t\n",
      " Epoch: [17][200/813]\tLoss 1.1635 (0.6769)\t\n",
      " Epoch: [17][400/813]\tLoss 0.3392 (0.6602)\t\n",
      " Epoch: [17][600/813]\tLoss 1.3307 (0.6651)\t\n",
      " Epoch: [17][800/813]\tLoss 0.4446 (0.6632)\t\n",
      " Epoch: [17][0/32]\tLoss 0.6092 (0.6092)\t\n",
      " Epoch: [18][0/813]\tLoss 0.3262 (0.3262)\t\n",
      " Epoch: [18][200/813]\tLoss 0.5004 (0.6490)\t\n",
      " Epoch: [18][400/813]\tLoss 0.5121 (0.6583)\t\n",
      " Epoch: [18][600/813]\tLoss 0.7933 (0.6589)\t\n",
      " Epoch: [18][800/813]\tLoss 0.5916 (0.6559)\t\n",
      " Epoch: [18][0/32]\tLoss 0.8651 (0.8651)\t\n",
      " Epoch: [19][0/813]\tLoss 0.6710 (0.6710)\t\n",
      " Epoch: [19][200/813]\tLoss 0.6833 (0.6441)\t\n",
      " Epoch: [19][400/813]\tLoss 0.4713 (0.6562)\t\n",
      " Epoch: [19][600/813]\tLoss 0.9148 (0.6562)\t\n",
      " Epoch: [19][800/813]\tLoss 0.6141 (0.6480)\t\n",
      " Epoch: [19][0/32]\tLoss 0.5211 (0.5211)\t\n",
      " Epoch: [20][0/813]\tLoss 0.4298 (0.4298)\t\n",
      " Epoch: [20][200/813]\tLoss 0.7421 (0.6225)\t\n",
      " Epoch: [20][400/813]\tLoss 0.1888 (0.6462)\t\n",
      " Epoch: [20][600/813]\tLoss 0.6555 (0.6454)\t\n",
      " Epoch: [20][800/813]\tLoss 0.9081 (0.6448)\t\n",
      " Epoch: [20][0/32]\tLoss 0.5529 (0.5529)\t\n",
      " Epoch: [21][0/813]\tLoss 0.5877 (0.5877)\t\n",
      " Epoch: [21][200/813]\tLoss 0.2203 (0.5900)\t\n",
      " Epoch: [21][400/813]\tLoss 0.8671 (0.6154)\t\n",
      " Epoch: [21][600/813]\tLoss 0.4094 (0.6233)\t\n",
      " Epoch: [21][800/813]\tLoss 0.3278 (0.6246)\t\n",
      " Epoch: [21][0/32]\tLoss 0.4736 (0.4736)\t\n",
      " Epoch: [22][0/813]\tLoss 0.3633 (0.3633)\t\n",
      " Epoch: [22][200/813]\tLoss 0.4928 (0.5812)\t\n",
      " Epoch: [22][400/813]\tLoss 0.3173 (0.6005)\t\n",
      " Epoch: [22][600/813]\tLoss 0.5548 (0.6050)\t\n",
      " Epoch: [22][800/813]\tLoss 0.8910 (0.6079)\t\n",
      " Epoch: [22][0/32]\tLoss 1.6725 (1.6725)\t\n",
      " Epoch: [23][0/813]\tLoss 0.5197 (0.5197)\t\n",
      " Epoch: [23][200/813]\tLoss 0.1817 (0.5824)\t\n",
      " Epoch: [23][400/813]\tLoss 0.6865 (0.5871)\t\n",
      " Epoch: [23][600/813]\tLoss 0.4756 (0.5933)\t\n",
      " Epoch: [23][800/813]\tLoss 0.6341 (0.5943)\t\n",
      " Epoch: [23][0/32]\tLoss 0.8248 (0.8248)\t\n",
      " Epoch: [24][0/813]\tLoss 0.9374 (0.9374)\t\n",
      " Epoch: [24][200/813]\tLoss 0.7254 (0.6112)\t\n",
      " Epoch: [24][400/813]\tLoss 1.0576 (0.5886)\t\n",
      " Epoch: [24][600/813]\tLoss 0.5797 (0.5942)\t\n",
      " Epoch: [24][800/813]\tLoss 0.7115 (0.5873)\t\n",
      " Epoch: [24][0/32]\tLoss 0.4027 (0.4027)\t\n",
      "early stopping.\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'device' : 'cuda:5',\n",
    "    'net_name' : 'vit_B_16_imagenet1k',\n",
    "    'dataset_name' : 'imagenet',\n",
    "    'dataset_args': {\n",
    "        'batch_size' : 16,\n",
    "        'classes': 10\n",
    "    },\n",
    "    'early_stopping_step' : 7,\n",
    "    'lr' : 0.0005,\n",
    "    'model_args' : {\n",
    "        'num_classes' : 10,\n",
    "        'image_size' : 224,\n",
    "        'in_channels' : 3,\n",
    "    },\n",
    "    'model_name' : 'B_16_imagenet1k',\n",
    "}\n",
    "\n",
    "main(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'device' : 'cuda:5',\n",
    "    'net_name' : 'vit_B_16_imagenet1k',\n",
    "    'dataset_name' : 'imagenet',\n",
    "    'dataset_args': {\n",
    "        'batch_size' : 16,\n",
    "        'classes': 10\n",
    "    },\n",
    "    'early_stopping_step' : 7,\n",
    "    'lr' : 0.0005,\n",
    "    'model_args' : {\n",
    "        'num_classes' : 10,\n",
    "        'image_size' : 224,\n",
    "        'in_channels' : 3,\n",
    "    },\n",
    "    'model_name' : 'B_16_imagenet1k',\n",
    "}\n",
    "\n",
    "for ratio in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "    args['reg_ratio']  = ratio\n",
    "    main(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'device' : 'cuda:5',\n",
    "    'net_name' : 'vit_B_16_imagenet1k',\n",
    "    'dataset_name' : 'cifar10',\n",
    "    'dataset_args': {\n",
    "        'batch_size' : 128,\n",
    "    },\n",
    "    'early_stopping_step' : 7,\n",
    "    'lr' : 0.0005,\n",
    "    'model_args' : {\n",
    "        'num_classes' : 10,\n",
    "        'image_size' : 32,\n",
    "        'in_channels' : 3,\n",
    "    },\n",
    "    'model_name' : 'B_16_imagenet1k',\n",
    "}\n",
    "\n",
    "for ratio in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "    args['reg_ratio']  = ratio\n",
    "    main(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
